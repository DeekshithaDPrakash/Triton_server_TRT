RoBERTa or any BERT models as of now doesn't support TensorRT-LLM.

So, using TensorRT and Triton servers separately is the only way.

How to make use of triton server for inferencing BERT based models in tensorrt format?
