This repo consists of guidelines to use Triton inference server suign TensorRT/TensorRT-LLM


Note:
1. For Solar models following Llama2 helps 
2. In case "rope_scaling" error pops up, upgrade the transformers to >=4.43 or 4.45.1
   
